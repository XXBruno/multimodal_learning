{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-27 13:57:24,780 : INFO : collecting all words and their counts\n",
      "2018-05-27 13:57:24,781 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2018-05-27 13:57:24,793 : INFO : collected 3702 word types and 10 unique tags from a corpus of 1503 examples and 65463 words\n",
      "2018-05-27 13:57:24,794 : INFO : Loading a fresh vocabulary\n",
      "2018-05-27 13:57:24,797 : INFO : min_count=5 retains 1058 unique words (28% of original 3702, drops 2644)\n",
      "2018-05-27 13:57:24,798 : INFO : min_count=5 leaves 61217 word corpus (93% of original 65463, drops 4246)\n",
      "2018-05-27 13:57:24,802 : INFO : deleting the raw counts dictionary of 3702 items\n",
      "2018-05-27 13:57:24,803 : INFO : sample=0.001 downsamples 71 most-common words\n",
      "2018-05-27 13:57:24,804 : INFO : downsampling leaves estimated 39737 word corpus (64.9% of prior 61217)\n",
      "2018-05-27 13:57:24,807 : INFO : estimated required memory for 1058 words and 400 dimensions: 3932600 bytes\n",
      "2018-05-27 13:57:24,808 : INFO : resetting layer weights\n",
      "2018-05-27 13:57:24,825 : INFO : training model with 1 workers on 1058 vocabulary and 400 features, using sg=1 hs=0 sample=0.001 negative=12 window=8\n",
      "2018-05-27 13:57:24,964 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-27 13:57:24,965 : INFO : EPOCH - 1 : training on 65463 raw words (41232 effective words) took 0.1s, 300207 effective words/s\n",
      "2018-05-27 13:57:25,103 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-27 13:57:25,104 : INFO : EPOCH - 2 : training on 65463 raw words (41111 effective words) took 0.1s, 298256 effective words/s\n",
      "2018-05-27 13:57:25,239 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-27 13:57:25,239 : INFO : EPOCH - 3 : training on 65463 raw words (41258 effective words) took 0.1s, 308456 effective words/s\n",
      "2018-05-27 13:57:25,374 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-27 13:57:25,375 : INFO : EPOCH - 4 : training on 65463 raw words (41138 effective words) took 0.1s, 308628 effective words/s\n",
      "2018-05-27 13:57:25,513 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-27 13:57:25,513 : INFO : EPOCH - 5 : training on 65463 raw words (41386 effective words) took 0.1s, 302660 effective words/s\n",
      "2018-05-27 13:57:25,648 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-27 13:57:25,649 : INFO : EPOCH - 6 : training on 65463 raw words (41225 effective words) took 0.1s, 308417 effective words/s\n",
      "2018-05-27 13:57:25,780 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-27 13:57:25,781 : INFO : EPOCH - 7 : training on 65463 raw words (41396 effective words) took 0.1s, 316671 effective words/s\n",
      "2018-05-27 13:57:25,912 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-27 13:57:25,912 : INFO : EPOCH - 8 : training on 65463 raw words (41267 effective words) took 0.1s, 318057 effective words/s\n",
      "2018-05-27 13:57:26,044 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-27 13:57:26,045 : INFO : EPOCH - 9 : training on 65463 raw words (41227 effective words) took 0.1s, 315618 effective words/s\n",
      "2018-05-27 13:57:26,175 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-27 13:57:26,175 : INFO : EPOCH - 10 : training on 65463 raw words (41149 effective words) took 0.1s, 319119 effective words/s\n",
      "2018-05-27 13:57:26,176 : INFO : training on a 654630 raw words (412389 effective words) took 1.3s, 305542 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>390</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.036712</td>\n",
       "      <td>0.125713</td>\n",
       "      <td>-0.124734</td>\n",
       "      <td>-0.106667</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>-0.050306</td>\n",
       "      <td>0.111168</td>\n",
       "      <td>0.089781</td>\n",
       "      <td>0.055605</td>\n",
       "      <td>-0.015501</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059164</td>\n",
       "      <td>0.095496</td>\n",
       "      <td>0.053163</td>\n",
       "      <td>0.076071</td>\n",
       "      <td>0.067306</td>\n",
       "      <td>0.084096</td>\n",
       "      <td>-0.133927</td>\n",
       "      <td>-0.074062</td>\n",
       "      <td>0.049969</td>\n",
       "      <td>0.115361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.096348</td>\n",
       "      <td>0.101196</td>\n",
       "      <td>0.102213</td>\n",
       "      <td>0.075164</td>\n",
       "      <td>0.074358</td>\n",
       "      <td>0.091330</td>\n",
       "      <td>0.093195</td>\n",
       "      <td>0.090551</td>\n",
       "      <td>0.098454</td>\n",
       "      <td>0.088012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113370</td>\n",
       "      <td>0.074758</td>\n",
       "      <td>0.087042</td>\n",
       "      <td>0.082179</td>\n",
       "      <td>0.075202</td>\n",
       "      <td>0.088117</td>\n",
       "      <td>0.065762</td>\n",
       "      <td>0.089179</td>\n",
       "      <td>0.078919</td>\n",
       "      <td>0.071315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.185695</td>\n",
       "      <td>-0.083176</td>\n",
       "      <td>-0.367557</td>\n",
       "      <td>-0.306175</td>\n",
       "      <td>-0.205887</td>\n",
       "      <td>-0.253398</td>\n",
       "      <td>-0.165538</td>\n",
       "      <td>-0.129377</td>\n",
       "      <td>-0.169600</td>\n",
       "      <td>-0.254812</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.315872</td>\n",
       "      <td>-0.083514</td>\n",
       "      <td>-0.134847</td>\n",
       "      <td>-0.150160</td>\n",
       "      <td>-0.194077</td>\n",
       "      <td>-0.120244</td>\n",
       "      <td>-0.305815</td>\n",
       "      <td>-0.290024</td>\n",
       "      <td>-0.121518</td>\n",
       "      <td>-0.066311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.027043</td>\n",
       "      <td>0.040351</td>\n",
       "      <td>-0.193342</td>\n",
       "      <td>-0.166150</td>\n",
       "      <td>-0.030403</td>\n",
       "      <td>-0.130954</td>\n",
       "      <td>0.044050</td>\n",
       "      <td>0.020416</td>\n",
       "      <td>-0.003923</td>\n",
       "      <td>-0.075329</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.143621</td>\n",
       "      <td>0.041695</td>\n",
       "      <td>-0.013343</td>\n",
       "      <td>0.022839</td>\n",
       "      <td>0.008424</td>\n",
       "      <td>0.026156</td>\n",
       "      <td>-0.175473</td>\n",
       "      <td>-0.143426</td>\n",
       "      <td>-0.016258</td>\n",
       "      <td>0.061593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.025975</td>\n",
       "      <td>0.114167</td>\n",
       "      <td>-0.147597</td>\n",
       "      <td>-0.108586</td>\n",
       "      <td>0.018958</td>\n",
       "      <td>-0.054181</td>\n",
       "      <td>0.123477</td>\n",
       "      <td>0.074383</td>\n",
       "      <td>0.040930</td>\n",
       "      <td>-0.007953</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065794</td>\n",
       "      <td>0.110099</td>\n",
       "      <td>0.032870</td>\n",
       "      <td>0.076104</td>\n",
       "      <td>0.069966</td>\n",
       "      <td>0.082599</td>\n",
       "      <td>-0.138166</td>\n",
       "      <td>-0.079054</td>\n",
       "      <td>0.048496</td>\n",
       "      <td>0.108382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.087997</td>\n",
       "      <td>0.217383</td>\n",
       "      <td>-0.080477</td>\n",
       "      <td>-0.049557</td>\n",
       "      <td>0.051316</td>\n",
       "      <td>0.031408</td>\n",
       "      <td>0.182338</td>\n",
       "      <td>0.160591</td>\n",
       "      <td>0.113455</td>\n",
       "      <td>0.039728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024163</td>\n",
       "      <td>0.149234</td>\n",
       "      <td>0.099935</td>\n",
       "      <td>0.126334</td>\n",
       "      <td>0.122239</td>\n",
       "      <td>0.126175</td>\n",
       "      <td>-0.102437</td>\n",
       "      <td>-0.003903</td>\n",
       "      <td>0.117288</td>\n",
       "      <td>0.168778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.329587</td>\n",
       "      <td>0.361999</td>\n",
       "      <td>0.156121</td>\n",
       "      <td>0.102557</td>\n",
       "      <td>0.222406</td>\n",
       "      <td>0.173797</td>\n",
       "      <td>0.336035</td>\n",
       "      <td>0.340312</td>\n",
       "      <td>0.337257</td>\n",
       "      <td>0.241919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211814</td>\n",
       "      <td>0.275730</td>\n",
       "      <td>0.328493</td>\n",
       "      <td>0.316998</td>\n",
       "      <td>0.301239</td>\n",
       "      <td>0.379097</td>\n",
       "      <td>0.105069</td>\n",
       "      <td>0.172675</td>\n",
       "      <td>0.245627</td>\n",
       "      <td>0.291606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0            1            2            3            4    \\\n",
       "count  1503.000000  1503.000000  1503.000000  1503.000000  1503.000000   \n",
       "mean      0.036712     0.125713    -0.124734    -0.106667     0.003139   \n",
       "std       0.096348     0.101196     0.102213     0.075164     0.074358   \n",
       "min      -0.185695    -0.083176    -0.367557    -0.306175    -0.205887   \n",
       "25%      -0.027043     0.040351    -0.193342    -0.166150    -0.030403   \n",
       "50%       0.025975     0.114167    -0.147597    -0.108586     0.018958   \n",
       "75%       0.087997     0.217383    -0.080477    -0.049557     0.051316   \n",
       "max       0.329587     0.361999     0.156121     0.102557     0.222406   \n",
       "\n",
       "               5            6            7            8            9    \\\n",
       "count  1503.000000  1503.000000  1503.000000  1503.000000  1503.000000   \n",
       "mean     -0.050306     0.111168     0.089781     0.055605    -0.015501   \n",
       "std       0.091330     0.093195     0.090551     0.098454     0.088012   \n",
       "min      -0.253398    -0.165538    -0.129377    -0.169600    -0.254812   \n",
       "25%      -0.130954     0.044050     0.020416    -0.003923    -0.075329   \n",
       "50%      -0.054181     0.123477     0.074383     0.040930    -0.007953   \n",
       "75%       0.031408     0.182338     0.160591     0.113455     0.039728   \n",
       "max       0.173797     0.336035     0.340312     0.337257     0.241919   \n",
       "\n",
       "          ...               390          391          392          393  \\\n",
       "count     ...       1503.000000  1503.000000  1503.000000  1503.000000   \n",
       "mean      ...         -0.059164     0.095496     0.053163     0.076071   \n",
       "std       ...          0.113370     0.074758     0.087042     0.082179   \n",
       "min       ...         -0.315872    -0.083514    -0.134847    -0.150160   \n",
       "25%       ...         -0.143621     0.041695    -0.013343     0.022839   \n",
       "50%       ...         -0.065794     0.110099     0.032870     0.076104   \n",
       "75%       ...          0.024163     0.149234     0.099935     0.126334   \n",
       "max       ...          0.211814     0.275730     0.328493     0.316998   \n",
       "\n",
       "               394          395          396          397          398  \\\n",
       "count  1503.000000  1503.000000  1503.000000  1503.000000  1503.000000   \n",
       "mean      0.067306     0.084096    -0.133927    -0.074062     0.049969   \n",
       "std       0.075202     0.088117     0.065762     0.089179     0.078919   \n",
       "min      -0.194077    -0.120244    -0.305815    -0.290024    -0.121518   \n",
       "25%       0.008424     0.026156    -0.175473    -0.143426    -0.016258   \n",
       "50%       0.069966     0.082599    -0.138166    -0.079054     0.048496   \n",
       "75%       0.122239     0.126175    -0.102437    -0.003903     0.117288   \n",
       "max       0.301239     0.379097     0.105069     0.172675     0.245627   \n",
       "\n",
       "               399  \n",
       "count  1503.000000  \n",
       "mean      0.115361  \n",
       "std       0.071315  \n",
       "min      -0.066311  \n",
       "25%       0.061593  \n",
       "50%       0.108382  \n",
       "75%       0.168778  \n",
       "max       0.291606  \n",
       "\n",
       "[8 rows x 400 columns]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# textual modality\n",
    "import doc2vec_wrapped\n",
    "textual_vectors = doc2vec_wrapped.vectorize_content()\n",
    "textual_vectors = pd.DataFrame.from_items(zip(textual_vectors.index, textual_vectors.values)).transpose()\n",
    "textual_vectors.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2038</th>\n",
       "      <th>2039</th>\n",
       "      <th>2040</th>\n",
       "      <th>2041</th>\n",
       "      <th>2042</th>\n",
       "      <th>2043</th>\n",
       "      <th>2044</th>\n",
       "      <th>2045</th>\n",
       "      <th>2046</th>\n",
       "      <th>2047</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "      <td>1503.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.355654</td>\n",
       "      <td>0.318166</td>\n",
       "      <td>0.291729</td>\n",
       "      <td>0.352245</td>\n",
       "      <td>0.385825</td>\n",
       "      <td>0.244915</td>\n",
       "      <td>0.237441</td>\n",
       "      <td>0.335135</td>\n",
       "      <td>0.283957</td>\n",
       "      <td>0.327072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.418530</td>\n",
       "      <td>0.426724</td>\n",
       "      <td>0.411744</td>\n",
       "      <td>0.355265</td>\n",
       "      <td>0.420206</td>\n",
       "      <td>0.395320</td>\n",
       "      <td>0.537359</td>\n",
       "      <td>0.373122</td>\n",
       "      <td>0.380587</td>\n",
       "      <td>0.393897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.298311</td>\n",
       "      <td>0.237947</td>\n",
       "      <td>0.253558</td>\n",
       "      <td>0.277936</td>\n",
       "      <td>0.292115</td>\n",
       "      <td>0.200343</td>\n",
       "      <td>0.217335</td>\n",
       "      <td>0.254297</td>\n",
       "      <td>0.214626</td>\n",
       "      <td>0.251209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408742</td>\n",
       "      <td>0.395654</td>\n",
       "      <td>0.357959</td>\n",
       "      <td>0.346029</td>\n",
       "      <td>0.411656</td>\n",
       "      <td>0.365647</td>\n",
       "      <td>0.467650</td>\n",
       "      <td>0.363158</td>\n",
       "      <td>0.361049</td>\n",
       "      <td>0.351719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001554</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004756</td>\n",
       "      <td>0.003344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.143822</td>\n",
       "      <td>0.139100</td>\n",
       "      <td>0.114142</td>\n",
       "      <td>0.142039</td>\n",
       "      <td>0.181004</td>\n",
       "      <td>0.104038</td>\n",
       "      <td>0.088657</td>\n",
       "      <td>0.138301</td>\n",
       "      <td>0.127651</td>\n",
       "      <td>0.140158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116164</td>\n",
       "      <td>0.134149</td>\n",
       "      <td>0.136428</td>\n",
       "      <td>0.095891</td>\n",
       "      <td>0.129346</td>\n",
       "      <td>0.125362</td>\n",
       "      <td>0.178187</td>\n",
       "      <td>0.099052</td>\n",
       "      <td>0.097790</td>\n",
       "      <td>0.116281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.277273</td>\n",
       "      <td>0.260632</td>\n",
       "      <td>0.220164</td>\n",
       "      <td>0.274736</td>\n",
       "      <td>0.315676</td>\n",
       "      <td>0.196037</td>\n",
       "      <td>0.176022</td>\n",
       "      <td>0.277499</td>\n",
       "      <td>0.226696</td>\n",
       "      <td>0.262550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297742</td>\n",
       "      <td>0.315017</td>\n",
       "      <td>0.315816</td>\n",
       "      <td>0.253461</td>\n",
       "      <td>0.300292</td>\n",
       "      <td>0.298466</td>\n",
       "      <td>0.417716</td>\n",
       "      <td>0.274856</td>\n",
       "      <td>0.265897</td>\n",
       "      <td>0.310238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.478862</td>\n",
       "      <td>0.436851</td>\n",
       "      <td>0.383518</td>\n",
       "      <td>0.496978</td>\n",
       "      <td>0.525515</td>\n",
       "      <td>0.331019</td>\n",
       "      <td>0.314601</td>\n",
       "      <td>0.464446</td>\n",
       "      <td>0.381805</td>\n",
       "      <td>0.451626</td>\n",
       "      <td>...</td>\n",
       "      <td>0.602347</td>\n",
       "      <td>0.620506</td>\n",
       "      <td>0.594137</td>\n",
       "      <td>0.514351</td>\n",
       "      <td>0.574434</td>\n",
       "      <td>0.551814</td>\n",
       "      <td>0.773599</td>\n",
       "      <td>0.530467</td>\n",
       "      <td>0.576464</td>\n",
       "      <td>0.575275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.975805</td>\n",
       "      <td>1.489835</td>\n",
       "      <td>1.968467</td>\n",
       "      <td>1.778700</td>\n",
       "      <td>2.346548</td>\n",
       "      <td>1.932409</td>\n",
       "      <td>1.690809</td>\n",
       "      <td>1.751645</td>\n",
       "      <td>1.642197</td>\n",
       "      <td>1.899562</td>\n",
       "      <td>...</td>\n",
       "      <td>2.726681</td>\n",
       "      <td>2.742829</td>\n",
       "      <td>2.146388</td>\n",
       "      <td>2.797326</td>\n",
       "      <td>2.461991</td>\n",
       "      <td>2.727641</td>\n",
       "      <td>2.810951</td>\n",
       "      <td>2.588238</td>\n",
       "      <td>2.122325</td>\n",
       "      <td>2.200421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 2048 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0            1            2            3            4     \\\n",
       "count  1503.000000  1503.000000  1503.000000  1503.000000  1503.000000   \n",
       "mean      0.355654     0.318166     0.291729     0.352245     0.385825   \n",
       "std       0.298311     0.237947     0.253558     0.277936     0.292115   \n",
       "min       0.000703     0.000000     0.000000     0.000000     0.001554   \n",
       "25%       0.143822     0.139100     0.114142     0.142039     0.181004   \n",
       "50%       0.277273     0.260632     0.220164     0.274736     0.315676   \n",
       "75%       0.478862     0.436851     0.383518     0.496978     0.525515   \n",
       "max       1.975805     1.489835     1.968467     1.778700     2.346548   \n",
       "\n",
       "              5            6            7            8            9     \\\n",
       "count  1503.000000  1503.000000  1503.000000  1503.000000  1503.000000   \n",
       "mean      0.244915     0.237441     0.335135     0.283957     0.327072   \n",
       "std       0.200343     0.217335     0.254297     0.214626     0.251209   \n",
       "min       0.000000     0.000000     0.000000     0.004756     0.003344   \n",
       "25%       0.104038     0.088657     0.138301     0.127651     0.140158   \n",
       "50%       0.196037     0.176022     0.277499     0.226696     0.262550   \n",
       "75%       0.331019     0.314601     0.464446     0.381805     0.451626   \n",
       "max       1.932409     1.690809     1.751645     1.642197     1.899562   \n",
       "\n",
       "          ...              2038         2039         2040         2041  \\\n",
       "count     ...       1503.000000  1503.000000  1503.000000  1503.000000   \n",
       "mean      ...          0.418530     0.426724     0.411744     0.355265   \n",
       "std       ...          0.408742     0.395654     0.357959     0.346029   \n",
       "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       ...          0.116164     0.134149     0.136428     0.095891   \n",
       "50%       ...          0.297742     0.315017     0.315816     0.253461   \n",
       "75%       ...          0.602347     0.620506     0.594137     0.514351   \n",
       "max       ...          2.726681     2.742829     2.146388     2.797326   \n",
       "\n",
       "              2042         2043         2044         2045         2046  \\\n",
       "count  1503.000000  1503.000000  1503.000000  1503.000000  1503.000000   \n",
       "mean      0.420206     0.395320     0.537359     0.373122     0.380587   \n",
       "std       0.411656     0.365647     0.467650     0.363158     0.361049   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.129346     0.125362     0.178187     0.099052     0.097790   \n",
       "50%       0.300292     0.298466     0.417716     0.274856     0.265897   \n",
       "75%       0.574434     0.551814     0.773599     0.530467     0.576464   \n",
       "max       2.461991     2.727641     2.810951     2.588238     2.122325   \n",
       "\n",
       "              2047  \n",
       "count  1503.000000  \n",
       "mean      0.393897  \n",
       "std       0.351719  \n",
       "min       0.000000  \n",
       "25%       0.116281  \n",
       "50%       0.310238  \n",
       "75%       0.575275  \n",
       "max       2.200421  \n",
       "\n",
       "[8 rows x 2048 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load image modality\n",
    "image_df = pd.read_json(\"data/inception_output.txt\", typ=\"series\", orient=\"records\", lines=True)\n",
    "i = image_df.apply(lambda e: list(e.keys())[0]).values\n",
    "image_vectors = pd.DataFrame.from_items(zip(i, image_df.apply(lambda e: np.array(list(e.values())[0])))).transpose()\n",
    "image_vectors = image_vectors.drop_duplicates()\n",
    "image_vectors.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "side_one_dim = len(image_vectors.iloc[0])\n",
    "side_two_dim = len(textual_vectors.iloc[0])\n",
    "\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 400  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(side_one_dim,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(int(side_one_dim/2), activation='relu')(input_img)\n",
    "encoded = Dense(int(side_one_dim/4), activation='relu')(encoded)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(side_two_dim, activation='sigmoid')(encoded)\n",
    "decoded = Dense(side_two_dim, activation='sigmoid')(decoded)\n",
    "decoded = Dense(side_two_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is our input placeholder\n",
    "rev_input_text = Input(shape=(side_two_dim,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "rev_encoded = Dense(side_two_dim, activation='sigmoid')(rev_input_text)\n",
    "rev_encoded = Dense(side_two_dim, activation='sigmoid')(rev_encoded)\n",
    "rev_encoded = Dense(encoding_dim, activation='sigmoid')(rev_encoded)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "rev_decoded = Dense(int(side_one_dim/4), activation='relu')(rev_encoded)\n",
    "rev_decoded = Dense(int(side_one_dim/2), activation='relu')(rev_decoded)\n",
    "rev_decoded = Dense(side_one_dim, activation='relu')(rev_decoded)\n",
    "\n",
    "rev_autoencoder = Model(rev_input_text, rev_decoded)\n",
    "rev_autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(side_one_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['000000149770.jpg', '000000043737.jpg', '000000550797.jpg',\n",
       "       '000000352584.jpg', '000000144798.jpg', '000000355677.jpg',\n",
       "       '000000151051.jpg', '000000244379.jpg', '000000179214.jpg',\n",
       "       '000000084674.jpg',\n",
       "       ...\n",
       "       '000000334417.jpg', '000000285894.jpg', '000000131273.jpg',\n",
       "       '000000567011.jpg', '000000574520.jpg', '000000081766.jpg',\n",
       "       '000000273715.jpg', '000000049810.jpg', '000000273642.jpg',\n",
       "       '000000225670.jpg'],\n",
       "      dtype='object', length=1202)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersecting_idx = textual_vectors.sample(frac=1).index.intersection(image_vectors.index)\n",
    "# train_idx = \n",
    "train_validate_split = 0.8\n",
    "\n",
    "train_idx = intersecting_idx[:int(len(intersecting_idx)*train_validate_split)]\n",
    "test_idx = intersecting_idx[int(len(intersecting_idx)*train_validate_split):]\n",
    "train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['000000491008.jpg', '000000402433.jpg', '000000025603.jpg',\n",
       "       '000000385029.jpg', '000000047112.jpg', '000000260105.jpg',\n",
       "       '000000306700.jpg', '000000179214.jpg', '000000357903.jpg',\n",
       "       '000000547854.jpg',\n",
       "       ...\n",
       "       '000000414170.jpg', '000000032735.jpg', '000000002532.jpg',\n",
       "       '000000131556.jpg', '000000154087.jpg', '000000447465.jpg',\n",
       "       '000000161781.jpg', '000000352760.jpg', '000000515577.jpg',\n",
       "       '000000301563.jpg'],\n",
       "      dtype='object', length=1503)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textual_vectors.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1202 samples, validate on 301 samples\n",
      "Epoch 1/100\n",
      "1202/1202 [==============================] - 0s - loss: 0.6702 - val_loss: 0.5625\n",
      "Epoch 2/100\n",
      "1202/1202 [==============================] - 0s - loss: 0.5091 - val_loss: 0.4270\n",
      "Epoch 3/100\n",
      "1202/1202 [==============================] - 0s - loss: 0.3856 - val_loss: 0.3218\n",
      "Epoch 4/100\n",
      "1202/1202 [==============================] - 0s - loss: 0.2898 - val_loss: 0.2410\n",
      "Epoch 5/100\n",
      "1202/1202 [==============================] - 0s - loss: 0.2169 - val_loss: 0.1802\n",
      "Epoch 6/100\n",
      "1202/1202 [==============================] - 0s - loss: 0.1622 - val_loss: 0.1346\n",
      "Epoch 7/100\n",
      "1202/1202 [==============================] - 0s - loss: 0.1208 - val_loss: 0.0997\n",
      "Epoch 8/100\n",
      "1202/1202 [==============================] - 0s - loss: 0.0888 - val_loss: 0.0720\n",
      "Epoch 9/100\n",
      "1202/1202 [==============================] - 0s - loss: 0.0632 - val_loss: 0.0494\n",
      "Epoch 10/100\n",
      "1202/1202 [==============================] - 0s - loss: 0.0419 - val_loss: 0.0301\n",
      "Epoch 11/100\n",
      "1202/1202 [==============================] - 0s - loss: 0.0236 - val_loss: 0.0133\n",
      "Epoch 12/100\n",
      "1202/1202 [==============================] - 0s - loss: 0.0074 - val_loss: -0.0020\n",
      "Epoch 13/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.0074 - val_loss: -0.0161\n",
      "Epoch 14/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.0212 - val_loss: -0.0295\n",
      "Epoch 15/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.0343 - val_loss: -0.0423\n",
      "Epoch 16/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.0470 - val_loss: -0.0548\n",
      "Epoch 17/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.0594 - val_loss: -0.0670\n",
      "Epoch 18/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.0716 - val_loss: -0.0792\n",
      "Epoch 19/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.0837 - val_loss: -0.0913\n",
      "Epoch 20/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.0958 - val_loss: -0.1035\n",
      "Epoch 21/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.1079 - val_loss: -0.1157\n",
      "Epoch 22/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.1202 - val_loss: -0.1279\n",
      "Epoch 23/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.1325 - val_loss: -0.1403\n",
      "Epoch 24/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.1449 - val_loss: -0.1528\n",
      "Epoch 25/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.1574 - val_loss: -0.1654\n",
      "Epoch 26/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.1701 - val_loss: -0.1782\n",
      "Epoch 27/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.1829 - val_loss: -0.1911\n",
      "Epoch 28/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.1958 - val_loss: -0.2041\n",
      "Epoch 29/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.2088 - val_loss: -0.2172\n",
      "Epoch 30/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.2220 - val_loss: -0.2305\n",
      "Epoch 31/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.2353 - val_loss: -0.2439\n",
      "Epoch 32/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.2487 - val_loss: -0.2573\n",
      "Epoch 33/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.2620 - val_loss: -0.2705\n",
      "Epoch 34/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.2750 - val_loss: -0.2834\n",
      "Epoch 35/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.2876 - val_loss: -0.2957\n",
      "Epoch 36/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.2998 - val_loss: -0.3075\n",
      "Epoch 37/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.3112 - val_loss: -0.3183\n",
      "Epoch 38/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.3213 - val_loss: -0.3277\n",
      "Epoch 39/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.3303 - val_loss: -0.3360\n",
      "Epoch 40/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.3381 - val_loss: -0.3434\n",
      "Epoch 41/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.3450 - val_loss: -0.3499\n",
      "Epoch 42/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.3512 - val_loss: -0.3559\n",
      "Epoch 43/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.3570 - val_loss: -0.3613\n",
      "Epoch 44/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.3621 - val_loss: -0.3660\n",
      "Epoch 45/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.3665 - val_loss: -0.3702\n",
      "Epoch 46/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.3707 - val_loss: -0.3743\n",
      "Epoch 47/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.3747 - val_loss: -0.3780\n",
      "Epoch 48/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.3782 - val_loss: -0.3812\n",
      "Epoch 49/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.3813 - val_loss: -0.3843\n",
      "Epoch 50/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.3843 - val_loss: -0.3870\n",
      "Epoch 51/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.3870 - val_loss: -0.3897\n",
      "Epoch 52/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.3895 - val_loss: -0.3919\n",
      "Epoch 53/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.3917 - val_loss: -0.3940\n",
      "Epoch 54/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.3936 - val_loss: -0.3957\n",
      "Epoch 55/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.3951 - val_loss: -0.3971\n",
      "Epoch 56/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.3966 - val_loss: -0.3985\n",
      "Epoch 57/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.3979 - val_loss: -0.3998\n",
      "Epoch 58/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.3991 - val_loss: -0.4008\n",
      "Epoch 59/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4001 - val_loss: -0.4018\n",
      "Epoch 60/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4010 - val_loss: -0.4027\n",
      "Epoch 61/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4019 - val_loss: -0.4035\n",
      "Epoch 62/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4026 - val_loss: -0.4042\n",
      "Epoch 63/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4033 - val_loss: -0.4049\n",
      "Epoch 64/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4040 - val_loss: -0.4056\n",
      "Epoch 65/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4047 - val_loss: -0.4063\n",
      "Epoch 66/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4053 - val_loss: -0.4069\n",
      "Epoch 67/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4059 - val_loss: -0.4074\n",
      "Epoch 68/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4064 - val_loss: -0.4080\n",
      "Epoch 69/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4070 - val_loss: -0.4085\n",
      "Epoch 70/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4075 - val_loss: -0.4090\n",
      "Epoch 71/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4080 - val_loss: -0.4095\n",
      "Epoch 72/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4084 - val_loss: -0.4099\n",
      "Epoch 73/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4089 - val_loss: -0.4104\n",
      "Epoch 74/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4093 - val_loss: -0.4109\n",
      "Epoch 75/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4098 - val_loss: -0.4113\n",
      "Epoch 76/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4102 - val_loss: -0.4118\n",
      "Epoch 77/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4106 - val_loss: -0.4121\n",
      "Epoch 78/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4110 - val_loss: -0.4125\n",
      "Epoch 79/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4113 - val_loss: -0.4129\n",
      "Epoch 80/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4117 - val_loss: -0.4132\n",
      "Epoch 81/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4120 - val_loss: -0.4135\n",
      "Epoch 82/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4123 - val_loss: -0.4138\n",
      "Epoch 83/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4126 - val_loss: -0.4141\n",
      "Epoch 84/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4129 - val_loss: -0.4144\n",
      "Epoch 85/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4132 - val_loss: -0.4146\n",
      "Epoch 86/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4134 - val_loss: -0.4149\n",
      "Epoch 87/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4136 - val_loss: -0.4151\n",
      "Epoch 88/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4139 - val_loss: -0.4154\n",
      "Epoch 89/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4141 - val_loss: -0.4156\n",
      "Epoch 90/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4143 - val_loss: -0.4158\n",
      "Epoch 91/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4145 - val_loss: -0.4160\n",
      "Epoch 92/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4147 - val_loss: -0.4162\n",
      "Epoch 93/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4149 - val_loss: -0.4164\n",
      "Epoch 94/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4151 - val_loss: -0.4166\n",
      "Epoch 95/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4153 - val_loss: -0.4168\n",
      "Epoch 96/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4155 - val_loss: -0.4170\n",
      "Epoch 97/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4157 - val_loss: -0.4172\n",
      "Epoch 98/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4158 - val_loss: -0.4174\n",
      "Epoch 99/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4160 - val_loss: -0.4175\n",
      "Epoch 100/100\n",
      "1202/1202 [==============================] - 0s - loss: -0.4162 - val_loss: -0.4177\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd867c0a898>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(image_vectors.loc[train_idx].values, textual_vectors.loc[train_idx].values,\n",
    "                epochs=100,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(image_vectors.loc[test_idx].values, textual_vectors.loc[test_idx].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1202 samples, validate on 301 samples\n",
      "Epoch 1/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.6628 - val_loss: 2.3877\n",
      "Epoch 2/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.4224 - val_loss: 2.3091\n",
      "Epoch 3/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.3157 - val_loss: 2.1339\n",
      "Epoch 4/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.1622 - val_loss: 2.0954\n",
      "Epoch 5/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.1439 - val_loss: 2.0916\n",
      "Epoch 6/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.1091 - val_loss: 2.0224\n",
      "Epoch 7/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.0687 - val_loss: 2.0184\n",
      "Epoch 8/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.0660 - val_loss: 2.0170\n",
      "Epoch 9/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.0649 - val_loss: 2.0163\n",
      "Epoch 10/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.0642 - val_loss: 2.0159\n",
      "Epoch 11/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.0638 - val_loss: 2.0156\n",
      "Epoch 12/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.0635 - val_loss: 2.0153\n",
      "Epoch 13/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.0632 - val_loss: 2.0151\n",
      "Epoch 14/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.0630 - val_loss: 2.0149\n",
      "Epoch 15/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.0620 - val_loss: 2.0084\n",
      "Epoch 16/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.0555 - val_loss: 2.0075\n",
      "Epoch 17/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.0549 - val_loss: 2.0073\n",
      "Epoch 18/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.0547 - val_loss: 2.0072\n",
      "Epoch 19/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.0546 - val_loss: 2.0071\n",
      "Epoch 20/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.0520 - val_loss: 1.9736\n",
      "Epoch 21/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.0115 - val_loss: 1.9588\n",
      "Epoch 22/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.0045 - val_loss: 1.9581\n",
      "Epoch 23/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.0040 - val_loss: 1.9579\n",
      "Epoch 24/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.0038 - val_loss: 1.9577\n",
      "Epoch 25/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.0036 - val_loss: 1.9576\n",
      "Epoch 26/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.0035 - val_loss: 1.9575\n",
      "Epoch 27/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.0034 - val_loss: 1.9574\n",
      "Epoch 28/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.0034 - val_loss: 1.9573\n",
      "Epoch 29/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.0033 - val_loss: 1.9573\n",
      "Epoch 30/100\n",
      "1202/1202 [==============================] - 0s - loss: 2.0016 - val_loss: 1.9545\n",
      "Epoch 31/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.9985 - val_loss: 1.9523\n",
      "Epoch 32/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.9980 - val_loss: 1.9523\n",
      "Epoch 33/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.9979 - val_loss: 1.9523\n",
      "Epoch 34/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.9978 - val_loss: 1.9522\n",
      "Epoch 35/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.9978 - val_loss: 1.9521\n",
      "Epoch 36/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.9977 - val_loss: 1.9521\n",
      "Epoch 37/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.9977 - val_loss: 1.9521\n",
      "Epoch 38/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.9656 - val_loss: 1.8880\n",
      "Epoch 39/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.9293 - val_loss: 1.8846\n",
      "Epoch 40/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.9279 - val_loss: 1.8842\n",
      "Epoch 41/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.9277 - val_loss: 1.8840\n",
      "Epoch 42/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.9275 - val_loss: 1.8840\n",
      "Epoch 43/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.9274 - val_loss: 1.8839\n",
      "Epoch 44/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.9273 - val_loss: 1.8837\n",
      "Epoch 45/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.9272 - val_loss: 1.8836\n",
      "Epoch 46/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.9271 - val_loss: 1.8836\n",
      "Epoch 47/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.9271 - val_loss: 1.8835\n",
      "Epoch 48/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.9270 - val_loss: 1.8835\n",
      "Epoch 49/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.9270 - val_loss: 1.8836\n",
      "Epoch 50/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.9269 - val_loss: 1.8835\n",
      "Epoch 51/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.9269 - val_loss: 1.8833\n",
      "Epoch 52/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.9269 - val_loss: 1.8833\n",
      "Epoch 53/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.9268 - val_loss: 1.8834\n",
      "Epoch 54/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.9268 - val_loss: 1.8833\n",
      "Epoch 55/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.9268 - val_loss: 1.8832\n",
      "Epoch 56/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.9267 - val_loss: 1.8833\n",
      "Epoch 57/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.9267 - val_loss: 1.8834\n",
      "Epoch 58/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.9107 - val_loss: 1.8544\n",
      "Epoch 59/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.8961 - val_loss: 1.8537\n",
      "Epoch 60/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.8957 - val_loss: 1.8535\n",
      "Epoch 61/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.8956 - val_loss: 1.8535\n",
      "Epoch 62/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.8955 - val_loss: 1.8534\n",
      "Epoch 63/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.8955 - val_loss: 1.8534\n",
      "Epoch 64/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.8954 - val_loss: 1.8534\n",
      "Epoch 65/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.8954 - val_loss: 1.8533\n",
      "Epoch 66/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.8954 - val_loss: 1.8534\n",
      "Epoch 67/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.8954 - val_loss: 1.8533\n",
      "Epoch 68/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.8953 - val_loss: 1.8534\n",
      "Epoch 69/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.8953 - val_loss: 1.8532\n",
      "Epoch 70/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.8953 - val_loss: 1.8532\n",
      "Epoch 71/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.8953 - val_loss: 1.8533\n",
      "Epoch 72/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.8953 - val_loss: 1.8534\n",
      "Epoch 73/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.8952 - val_loss: 1.8532\n",
      "Epoch 74/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.8952 - val_loss: 1.8423\n",
      "Epoch 75/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.8738 - val_loss: 1.8285\n",
      "Epoch 76/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.8692 - val_loss: 1.8281\n",
      "Epoch 77/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.8690 - val_loss: 1.8280\n",
      "Epoch 78/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.8689 - val_loss: 1.8279\n",
      "Epoch 79/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.8689 - val_loss: 1.8279\n",
      "Epoch 80/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.8688 - val_loss: 1.8277\n",
      "Epoch 81/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.8460 - val_loss: 1.7630\n",
      "Epoch 82/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.8004 - val_loss: 1.7584\n",
      "Epoch 83/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.7979 - val_loss: 1.7424\n",
      "Epoch 84/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.7762 - val_loss: 1.7341\n",
      "Epoch 85/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.7730 - val_loss: 1.7335\n",
      "Epoch 86/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.7726 - val_loss: 1.7272\n",
      "Epoch 87/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.7627 - val_loss: 1.7220\n",
      "Epoch 88/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.7607 - val_loss: 1.7218\n",
      "Epoch 89/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.7605 - val_loss: 1.7218\n",
      "Epoch 90/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.7604 - val_loss: 1.7217\n",
      "Epoch 91/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.7603 - val_loss: 1.7213\n",
      "Epoch 92/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.7602 - val_loss: 1.7214\n",
      "Epoch 93/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.7601 - val_loss: 1.7212\n",
      "Epoch 94/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.7600 - val_loss: 1.7212\n",
      "Epoch 95/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.7600 - val_loss: 1.7211\n",
      "Epoch 96/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.7599 - val_loss: 1.7211\n",
      "Epoch 97/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.7599 - val_loss: 1.7211\n",
      "Epoch 98/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.7598 - val_loss: 1.7210\n",
      "Epoch 99/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.7597 - val_loss: 1.7208\n",
      "Epoch 100/100\n",
      "1202/1202 [==============================] - 0s - loss: 1.7597 - val_loss: 1.7209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd863e61208>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_autoencoder.fit(textual_vectors.loc[train_idx].values, image_vectors.loc[train_idx].values,\n",
    "                epochs=100,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(textual_vectors.loc[test_idx].values, image_vectors.loc[test_idx].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def generate_latent_space(forward_autoencoder, backward_autoencoder, forward_data, backward_data):\n",
    "    inp = autoencoder.input\n",
    "\n",
    "    outputs = [layer.output for layer in autoencoder.layers]          # all layer outputs\n",
    "    functor = K.function([inp]+ [K.learning_phase()], outputs ) # evaluation function\n",
    "\n",
    "    # Testing\n",
    "    layer_outs = functor([forward_data, 1.])\n",
    "\n",
    "    compressed_layer_activations = layer_outs[3]\n",
    "    \n",
    "    # reverse activations:\n",
    "    inp = rev_autoencoder.input\n",
    "\n",
    "    outputs = [layer.output for layer in rev_autoencoder.layers]          # all layer outputs\n",
    "    functor = K.function([inp]+ [K.learning_phase()], outputs ) # evaluation function\n",
    "\n",
    "    # Testing\n",
    "    layer_outs = functor([backward_data, 1.])\n",
    "\n",
    "    rev_compressed_layer_activations = layer_outs[3]\n",
    "    \n",
    "    print(\"Forward:\")\n",
    "    print(compressed_layer_activations)\n",
    "    print(\"Backward:\")\n",
    "    print(rev_compressed_layer_activations)\n",
    "    \n",
    "#     averaged_activations = np.mean([compressed_layer_activations, rev_compressed_layer_activations], axis=0)\n",
    "#     print(\"Averaged:\")\n",
    "#     print(averaged_activations)\n",
    "    \n",
    "#     return averaged_activations\n",
    "    concatenated_activations = np.concatenate([compressed_layer_activations, rev_compressed_layer_activations], axis=1)\n",
    "    print(\"Concatenated:\")\n",
    "    print(concatenated_activations)\n",
    "    return concatenated_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward:\n",
      "[[ 5.79345369  2.57230401  0.         ...,  0.39666089  0.          5.23416138]\n",
      " [ 3.42499661  1.64846647  0.         ...,  0.44016919  0.          3.19248557]\n",
      " [ 3.40518165  1.73569441  0.         ...,  0.24984393  0.          3.29419351]\n",
      " ..., \n",
      " [ 3.96296358  1.53496897  0.         ...,  0.19782898  0.          3.48514438]\n",
      " [ 4.45099688  1.97926724  0.         ...,  0.35071132  0.          4.03781128]\n",
      " [ 5.80035067  2.29949737  0.         ...,  0.87284786  0.          4.43661022]]\n",
      "Backward:\n",
      "[[ 0.80457872  0.58818638  0.44404337 ...,  0.51560038  0.33974555\n",
      "   0.64611423]\n",
      " [ 0.80280048  0.58695912  0.44507906 ...,  0.51610607  0.33787477\n",
      "   0.64815736]\n",
      " [ 0.80332798  0.58866459  0.44547221 ...,  0.51573581  0.3387728\n",
      "   0.64631784]\n",
      " ..., \n",
      " [ 0.80373734  0.58903533  0.44721588 ...,  0.51449448  0.33836007\n",
      "   0.64760548]\n",
      " [ 0.80371577  0.58921856  0.44692591 ...,  0.51452821  0.33895931\n",
      "   0.64807165]\n",
      " [ 0.80444378  0.58840048  0.44471106 ...,  0.51641053  0.33808595\n",
      "   0.64755905]]\n",
      "Concatenated:\n",
      "[[ 5.79345369  2.57230401  0.         ...,  0.51560038  0.33974555\n",
      "   0.64611423]\n",
      " [ 3.42499661  1.64846647  0.         ...,  0.51610607  0.33787477\n",
      "   0.64815736]\n",
      " [ 3.40518165  1.73569441  0.         ...,  0.51573581  0.3387728\n",
      "   0.64631784]\n",
      " ..., \n",
      " [ 3.96296358  1.53496897  0.         ...,  0.51449448  0.33836007\n",
      "   0.64760548]\n",
      " [ 4.45099688  1.97926724  0.         ...,  0.51452821  0.33895931\n",
      "   0.64807165]\n",
      " [ 5.80035067  2.29949737  0.         ...,  0.51641053  0.33808595\n",
      "   0.64755905]]\n",
      "Forward:\n",
      "[[ 5.68963575  2.39380097  0.         ...,  0.60848707  0.          4.67444372]\n",
      " [ 3.19540286  1.48851585  0.         ...,  0.15316567  0.          3.11055613]\n",
      " [ 2.57874489  1.17278969  0.         ...,  0.          0.          2.29646564]\n",
      " ..., \n",
      " [ 2.11202788  1.13454735  0.         ...,  0.12299678  0.          2.08921361]\n",
      " [ 3.25698662  1.25884748  0.         ...,  0.58987826  0.          3.00208354]\n",
      " [ 2.22109699  1.32625926  0.         ...,  0.15777394  0.          2.0873096 ]]\n",
      "Backward:\n",
      "[[ 0.80264205  0.58830458  0.44600677 ...,  0.51636517  0.33777961\n",
      "   0.64785421]\n",
      " [ 0.80442297  0.58719778  0.44308615 ...,  0.51726055  0.33962956\n",
      "   0.64710498]\n",
      " [ 0.80394274  0.58909184  0.44547358 ...,  0.5188511   0.33743259\n",
      "   0.64618331]\n",
      " ..., \n",
      " [ 0.80397618  0.58705914  0.44414982 ...,  0.5158962   0.33715534\n",
      "   0.64684325]\n",
      " [ 0.8037765   0.58704126  0.44437331 ...,  0.51538825  0.34081861\n",
      "   0.64866084]\n",
      " [ 0.80436927  0.58683664  0.44359076 ...,  0.51700193  0.3374539\n",
      "   0.64624125]]\n",
      "Concatenated:\n",
      "[[ 5.68963575  2.39380097  0.         ...,  0.51636517  0.33777961\n",
      "   0.64785421]\n",
      " [ 3.19540286  1.48851585  0.         ...,  0.51726055  0.33962956\n",
      "   0.64710498]\n",
      " [ 2.57874489  1.17278969  0.         ...,  0.5188511   0.33743259\n",
      "   0.64618331]\n",
      " ..., \n",
      " [ 2.11202788  1.13454735  0.         ...,  0.5158962   0.33715534\n",
      "   0.64684325]\n",
      " [ 3.25698662  1.25884748  0.         ...,  0.51538825  0.34081861\n",
      "   0.64866084]\n",
      " [ 2.22109699  1.32625926  0.         ...,  0.51700193  0.3374539\n",
      "   0.64624125]]\n"
     ]
    }
   ],
   "source": [
    "train_latent = generate_latent_space(autoencoder, rev_autoencoder, \n",
    "                                     image_vectors.loc[train_idx], textual_vectors.loc[train_idx])\n",
    "test_latent = generate_latent_space(autoencoder, rev_autoencoder, \n",
    "                                    image_vectors.loc[test_idx], textual_vectors.loc[test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1202, 800)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "000000491008.jpg    pizza\n",
       "000000402433.jpg    pizza\n",
       "000000025603.jpg    pizza\n",
       "000000385029.jpg    pizza\n",
       "000000047112.jpg    pizza\n",
       "000000260105.jpg    pizza\n",
       "000000306700.jpg    pizza\n",
       "000000179214.jpg    pizza\n",
       "000000357903.jpg    pizza\n",
       "000000547854.jpg    pizza\n",
       "dtype: object"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the classifier over the latent space: logistic regression + fully connected neural net\n",
    "base_df = pd.read_json(\"data/COCO/coco-easier.txt\", lines=True)\n",
    "categories = pd.Series(base_df[\"category\"].values, index=base_df[\"file_name\"].values)\n",
    "categories.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Returns the mean accuracy on the given test data and labels, in 5 cross validation splits\n",
    "scores = cross_val_score(classifier, \n",
    "                         pd.DataFrame(df[\"vectors\"].tolist()), \n",
    "                         df[\"category\"].values, \n",
    "                         cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(train_latent, categories[train_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(test_latent, categories[test_idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
